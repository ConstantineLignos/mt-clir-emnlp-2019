{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['dataset', 'lang', 'size', 'bpe.tokens', 'model', 'query', 'map',\n",
       "       'map.gm', 'p.5', 'p.10', 'r.10', 'rbo', 'ndcg', 'bleu.lower',\n",
       "       'bleu.lower.stem', 'bleu.lower.depunc', 'bleu.lower.stem.depunc',\n",
       "       'prec1.lower', 'prec1.lower.stem', 'prec1.lower.depunc',\n",
       "       'prec1.lower.stem.depunc', 'prec2.lower', 'prec2.lower.stem',\n",
       "       'prec2.lower.depunc', 'prec2.lower.stem.depunc', 'prec3.lower',\n",
       "       'prec3.lower.stem', 'prec3.lower.depunc', 'prec3.lower.stem.depunc',\n",
       "       'prec4.lower', 'prec4.lower.stem', 'prec4.lower.depunc',\n",
       "       'prec4.lower.stem.depunc'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "scores = pd.read_table('mt_ir_scores_all.tsv')\n",
    "scores.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute pairwise metrics correlations\n",
    "corr_dfs = []\n",
    "for lang in scores.lang.unique():\n",
    "    for dataset in scores.dataset.unique():\n",
    "        dataset_scores = scores[(scores.lang == lang) & (scores.dataset == dataset)]\n",
    "        for model in dataset_scores.model.unique():\n",
    "            model_scores = dataset_scores[dataset_scores.model == model]\n",
    "            common_fields = {'dataset': [dataset], 'lang': [lang], 'model': [model]}\n",
    "            model_corr = model_scores.corr('kendall')\n",
    "            for measure1, row in model_corr.items():\n",
    "                for measure2, value, in row.items():\n",
    "                    if measure1 != measure2:\n",
    "                        row_dict = dict(common_fields)\n",
    "                        row_dict.update({'measure1': [measure1], 'measure2': [measure2], 'corr': [value]})\n",
    "                        corr_dfs.append(pd.DataFrame(row_dict))\n",
    "                            \n",
    "corrs = pd.concat(corr_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out correlations\n",
    "corrs.to_csv('mt_ir_scores_corr.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "europarl bm25 hypothesis: BPE 32k != BPE 16k\n",
      "Mean 32k - 16k 0.009574999999999997\n",
      "WilcoxonResult(statistic=24.0, pvalue=0.02280365600647676)\n",
      "\n",
      "wiki bm25 hypothesis: BPE 32k != BPE 16k\n",
      "Mean 32k - 16k -0.056543749999999976\n",
      "WilcoxonResult(statistic=0.0, pvalue=0.00043680902321148305)\n",
      "\n",
      "wiki neural hypothesis: BPE 32k != BPE 16k\n",
      "Mean 32k - 16k -0.010381250000000002\n",
      "WilcoxonResult(statistic=4.0, pvalue=0.0009350911193154891)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare BPE for each model and dataset\n",
    "def condition_key_size(row):\n",
    "    return (row.model, row.lang, row['size'])\n",
    "\n",
    "def mean_diff(seq1, seq2):\n",
    "    assert len(seq1) == len(seq2)\n",
    "    return sum(s1 - s2 for s1, s2 in zip(seq1, seq2)) / len(seq1)\n",
    "\n",
    "def median_diff(seq1, seq2):\n",
    "    assert len(seq1) == len(seq2)\n",
    "    return np.median(seq1) - np.median(seq2)\n",
    "\n",
    "for dataset in scores.dataset.unique():\n",
    "    dataset_scores = scores[scores.dataset == dataset]\n",
    "    for model in dataset_scores.model.unique():\n",
    "        condition_bpe_scores = defaultdict(dict)\n",
    "        for _, row in dataset_scores[dataset_scores.model == model].iterrows():\n",
    "            assert len(condition_bpe_scores[condition_key_size(row)]) < 2\n",
    "            condition_bpe_scores[condition_key_size(row)][row['bpe.tokens']] = row['map']\n",
    "\n",
    "        bpe16k, bpe32k = zip(*((condition_scores[16000], condition_scores[32000]) for condition_scores in condition_bpe_scores.values()))\n",
    "        test = stats.wilcoxon(bpe32k, bpe16k)\n",
    "        print(dataset, model, 'hypothesis: BPE 32k != BPE 16k')\n",
    "        print('Mean 32k - 16k', mean_diff(bpe32k, bpe16k))\n",
    "        print(test)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WilcoxonResult(statistic=0.0, pvalue=7.74421643104407e-06)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.wilcoxon(list(range(20, 40)), list(range(20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suire,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:saral]",
   "language": "python",
   "name": "conda-env-saral-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
